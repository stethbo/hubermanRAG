{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810da4be",
   "metadata": {},
   "source": [
    "## What is langchain?\n",
    "\n",
    "LangChain is a framework designed to build applications powered by large language models (LLMs), enabling dynamic interactions with tools, APIs, and data sources. It simplifies the development of AI-driven systems like chatbots, autonomous agents, and data processing pipelines by integrating language models seamlessly into workflows. </br> </br>\n",
    "![LangChain Icon](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQlnc1hwPoxs1LQSbeAIk_JfVEwONTMLDrdRA&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fef1ad",
   "metadata": {},
   "source": [
    "### Why do we use langchain using AWS?\n",
    "Combining LangChain with AWS allows developers to leverage LangChain's powerful LLM framework alongside AWS's scalable infrastructure, enabling seamless deployment, efficient resource management, and integration with AWS services like Lambda, SageMaker, and DynamoDB for building robust, production-grade AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56770dd7",
   "metadata": {},
   "source": [
    "## How to use Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6dbc8",
   "metadata": {},
   "source": [
    "#### Installing necesseary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b73ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv \\\n",
    "    boto3 \\\n",
    "    langchain_community \\\n",
    "    langchain_experimental \\\n",
    "    presidio-anonymizer \\\n",
    "    presidio-analyzer \\\n",
    "    poppler-utils \\\n",
    "    neo4j \\\n",
    "    wget \\\n",
    "    pymupdf \\\n",
    "    langchain_aws \\\n",
    "    chromadb \\\n",
    "    json-repair \\\n",
    "    pandas \\\n",
    "    numexpr \\\n",
    "    duckduckgo-search \\\n",
    "    Faker -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb4830",
   "metadata": {},
   "source": [
    "#### Importing necesseary libraries and setting eviromental variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b49367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export AWS_PROFILE=stormy-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81417cb-8ec5-4864-9e6c-8529beada5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stormy-api\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import os\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "# load_dotenv()\n",
    "AWS_PROFILE = 'stormy-api'\n",
    "print(AWS_PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c5687",
   "metadata": {},
   "source": [
    "#### How to define which LLM to use using Langchain?\n",
    "\n",
    "In order to define the LLM we will be using, we need to know 2 things beforehand. <br/>\n",
    "First, what is provider of that particular LLM. The providers are listed on [the langchain website](https://python.langchain.com/docs/integrations/providers/). <br/>\n",
    "For the purpose of this tutorial we will use Bedrock, as this provided has been decided to be primary one for Dolby. <br/>\n",
    "The second issue is the model we can use. This involves model name and its version. In order to make solutions create by LLM's more backward comparible, a lot of providers allow usage of older versions of their top models. </br>\n",
    "To find out proper model id for configurating chosen chat model, for Bedrock service, visit [their website](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abe48056-8a6d-4a47-a76b-2b0ad7e883af",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "# session = boto3.Session()\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "bedrock = session.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "bedrock_runtime = session.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=\"us-west-2\"\n",
    ")\n",
    "llm = BedrockChat(model_id=model_id, client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126adb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Session(region_name='us-west-2')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158b52f",
   "metadata": {},
   "source": [
    "### How to use Langchain?\n",
    "This section will be devoted to showing how to use basic Langchain and prompt techniques to develop applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2af92a",
   "metadata": {},
   "source": [
    "#### How to run Inference?\n",
    "The most basic of the tasks consists of simply asking LLM a question that based on its training data is likely to give correct answer. Examples of tasks that are \"baked\" into llm's are:\n",
    "\n",
    "1. **Text Summarization**  \n",
    "   Generate concise summaries of long texts without requiring training on the specific dataset.\n",
    "\n",
    "2. **Sentiment Analysis**  \n",
    "   Determine the sentiment (positive, negative, neutral) of a given text input.\n",
    "\n",
    "3. **Language Translation**  \n",
    "   Translate text between languages without prior fine-tuning for the target pair.\n",
    "\n",
    "4. **Content Classification**  \n",
    "   Categorize text into predefined categories based on context and semantics.\n",
    "\n",
    "5. **Question Answering**  \n",
    "   Provide accurate answers to user queries based on context or provided documents.\n",
    "\n",
    "6. **Text Generation**  \n",
    "   Produce coherent and contextually relevant text based on a prompt.\n",
    "\n",
    "7. **Named Entity Recognition (NER)**  \n",
    "   Identify and extract entities such as names, dates, and locations from text.\n",
    "\n",
    "8. **Code Generation**  \n",
    "   Write syntactically correct code snippets based on plain-text descriptions or specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1da522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise summary:\n",
      "\n",
      "LangChain is a framework for creating language model-powered applications, offering tools to connect models with external data and develop sophisticated applications.\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Example 1: Text Summarization\n",
    "summarization_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Please provide a concise summary of the following text: {text}\"\n",
    ")\n",
    "summarization_chain =  summarization_prompt | llm\n",
    "\n",
    "# Input example for summarization\n",
    "input_text = \"LangChain is a framework for developing applications powered by large language models. It provides tools to integrate models with external data sources and build advanced applications.\"\n",
    "summary = summarization_chain.invoke(input_text)\n",
    "print(summary.content)\n",
    "\n",
    "# Example 2: Sentiment Analysis\n",
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Determine the sentiment of the following text (positive, negative, or neutral). Provide no additional explanation, just a label: {text}\"\n",
    ")\n",
    "sentiment_chain = sentiment_prompt | llm\n",
    "\n",
    "# Input example for sentiment analysis\n",
    "sentiment_text = \"I absolutely love the new features introduced in this product!\"\n",
    "sentiment = sentiment_chain.invoke(sentiment_text)\n",
    "print(\"Sentiment:\", sentiment.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ee086",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "In order to provide user with real time output generation, it is recommended to use streaming feature, especially when expexting large outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9056e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's an example of a simple Streamlit dashboard that includes a title, some text, a chart, and a user input field:\n",
      "\n",
      "```python\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Set page title\n",
      "st.set_page_config(page_title=\"Simple Dashboard\")\n",
      "\n",
      "# Add a title\n",
      "st.title(\"Simple Streamlit Dashboard\")\n",
      "\n",
      "# Add some text\n",
      "st.write(\"This is a simple example of a Streamlit dashboard.\")\n",
      "\n",
      "# Create some sample data\n",
      "data = pd.DataFrame({\n",
      "    'x': range(10),\n",
      "    'y': np.random.randn(10)\n",
      "})\n",
      "\n",
      "# Create a line chart\n",
      "st.subheader(\"Line Chart\")\n",
      "st.line_chart(data)\n",
      "\n",
      "# Create a bar chart\n",
      "st.subheader(\"Bar Chart\")\n",
      "fig, ax = plt.subplots()\n",
      "ax.bar(data['x'], data['y'])\n",
      "st.pyplot(fig)\n",
      "\n",
      "# Add a slider\n",
      "st.subheader(\"Slider Input\")\n",
      "number = st.slider(\"Select a number\", 0, 100, 50)\n",
      "st.write(f\"You selected: {number}\")\n",
      "\n",
      "# Add a text input\n",
      "st.subheader(\"Text Input\")\n",
      "user_input = st.text_input(\"Enter some text\")\n",
      "if user_input:\n",
      "    st.write(f\"You entered: {user_input}\")\n",
      "\n",
      "# Add a selectbox\n",
      "st.subheader(\"Select Box\")\n",
      "option = st.selectbox(\n",
      "    'What is your favorite color?',\n",
      "    ('Red', 'Green', 'Blue'))\n",
      "st.write(f'Your favorite color is {option}')\n",
      "\n",
      "# Add a checkbox\n",
      "st.subheader(\"Checkbox\")\n",
      "if st.checkbox('Show raw data'):\n",
      "    st.subheader('Raw data')\n",
      "    st.write(data)\n",
      "```\n",
      "\n",
      "To run this dashboard, you'll need to have Streamlit installed (`pip install streamlit`), as well as pandas, numpy, and matplotlib.\n",
      "\n",
      "Save this code in a file (e.g., `dashboard.py`), then run it from the command line with:\n",
      "\n",
      "```\n",
      "streamlit run dashboard.py\n",
      "```\n",
      "\n",
      "This will start a local server and open the dashboard in your default web browser. The dashboard includes:\n",
      "\n",
      "1. A title and some introductory text\n",
      "2. A line chart and a bar chart using sample data\n",
      "3. A slider for numerical input\n",
      "4. A text input field\n",
      "5. A select box for choosing from predefined options\n",
      "6. A checkbox that, when checked, displays the raw data used for the charts\n",
      "\n",
      "This example demonstrates some of Streamlit's basic features. You can expand on this to create more complex dashboards with more interactivity and data visualization as needed."
     ]
    }
   ],
   "source": [
    "question = \"Give me an example of streamlit code for a simple dashboard.\"\n",
    "for chunk in llm.stream(question):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c17861",
   "metadata": {},
   "source": [
    "### Batching\n",
    "To optimize processing time and handle multiple inputs efficiently, batching is recommended. This approach is especially useful when analyzing large datasets or executing repeated tasks, as it allows multiple inputs to be processed in parallel, reducing latency and improving throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c733e753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I absolutely love the new features introduced in this product!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The user experience was horrible and frustrating.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: This is an average product; nothing special but not terrible either.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Fantastic! I'm thrilled with this service.\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_texts = [\n",
    "    \"I absolutely love the new features introduced in this product!\",\n",
    "    \"The user experience was horrible and frustrating.\",\n",
    "    \"This is an average product; nothing special but not terrible either.\",\n",
    "    \"Fantastic! I'm thrilled with this service.\",\n",
    "]\n",
    "\n",
    "# Use invoke_batch for processing multiple texts\n",
    "batch_sentiments = sentiment_chain.batch(batch_texts)\n",
    "\n",
    "# Display results\n",
    "for text, sentiment in zip(batch_texts, batch_sentiments):\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f1349",
   "metadata": {},
   "source": [
    "### More advanced prompting techniques\n",
    "This part of the tutorial will be dedicated to more avanced prompting techniques, that allow for more predice and grounded answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cd78a",
   "metadata": {},
   "source": [
    "#### Few shot propting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61329003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a science fiction-themed poem about music:\n",
      "\n",
      "Cosmic Symphony\n",
      "\n",
      "In the void between stars, a melody grows\n",
      "Pulsars keep time, as the universe flows\n",
      "Quantum strings vibrate, an ethereal song\n",
      "Echoing through space, where no one belongs\n",
      "\n",
      "Alien harmonies, beyond human ears\n",
      "Resonate through eons, across light-years\n",
      "Plasma harps played by solar winds\n",
      "A galactic orchestra, as dimensions spin\n",
      "\n",
      "Nebulae hum with celestial tunes\n",
      "Asteroids percussion, comets swoon\n",
      "Black holes conduct with gravity's might\n",
      "An eternal concert in the endless night\n",
      "\n",
      "Spacefarers listen, their hearts enthralled\n",
      "By cosmic music, unheard by all\n",
      "In this vast expanse, cold and dark\n",
      "Beats the rhythm of creation's spark\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Define the system message and sample dialoguesd\n",
    "system_message = \"Consider yourself as a seasoned novelist.\"\n",
    "\n",
    "user_dialogue1 = \"Fantasy novel, opening scene. Describe a mystical forest filled with magical creatures.\"\n",
    "sample_response1 = \"\"\"The forest was alive with wonder. Golden leaves rustled softly in the breeze, their edges glinting like precious metals.\n",
    "Strange, luminous flowers cast a gentle glow, revealing the silhouettes of creatures unseen in the daylight.\n",
    "A unicorn with a shimmering coat moved gracefully between the trees, while tiny fairies darted about, their laughter like the tinkling of bells.\"\"\"\n",
    "\n",
    "user_dialogue2 = \"Thriller, a tense chase through a crowded city market.\"\n",
    "sample_response2 = \"\"\"The market was a blur of colors and sounds.\n",
    "People shouted, bartering over fresh produce and exotic goods.\n",
    "Amidst the chaos, a man in a dark coat pushed his way through, glancing back frequently.\n",
    "Behind him, a figure moved with relentless precision, their eyes locked on the target.\n",
    "The air was thick with tension, every footstep echoing the urgency of the chase.\"\"\"\n",
    "\n",
    "user_dialogue3 = \"Romantic drama, a heartfelt confession at a picturesque beach.\"\n",
    "sample_response3 = \"\"\"The sun was setting, casting a warm golden glow over the serene beach.\n",
    "Waves lapped gently at the shore, creating a soothing soundtrack to the evening.\n",
    "Two figures stood facing each other, the world around them fading away.\n",
    "'I’ve loved you since the first moment I saw you,' one confessed, their voice trembling with emotion.\n",
    "The other’s eyes filled with tears, the weight of unspoken feelings finally finding release.\"\"\"\n",
    "\n",
    "examples = [\n",
    "    {\"input\": user_dialogue1, \"output\": sample_response1},\n",
    "    {\"input\": user_dialogue2, \"output\": sample_response2},\n",
    "    {\"input\": user_dialogue3, \"output\": sample_response3},\n",
    "]\n",
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | llm\n",
    "\n",
    "print(\n",
    "    chain.invoke({\"input\": \"Scify, music, poem\"}).content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613673ec",
   "metadata": {},
   "source": [
    "### Chain of Though Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d8a048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the problems we need to solve:\n",
      "\n",
      "1. Identify the film with a bride seeking revenge and its soundtrack.\n",
      "2. Determine the lead actress in this film.\n",
      "3. Find out the hometown of this lead actress.\n",
      "4. Identify the famous basketball team in this hometown.\n",
      "5. Determine the second name of this basketball team.\n",
      "\n",
      "Now, let's solve each sub-problem:\n",
      "\n",
      "1. Identify the film with a bride seeking revenge and its soundtrack:\n",
      "   The most famous film fitting this description is \"Kill Bill\" directed by Quentin Tarantino. The soundtrack features several iconic songs.\n",
      "\n",
      "2. Determine the lead actress in this film:\n",
      "   The lead actress in \"Kill Bill\" is Uma Thurman.\n",
      "\n",
      "3. Find out the hometown of this lead actress:\n",
      "   Uma Thurman was born in Boston, Massachusetts, USA.\n",
      "\n",
      "4. Identify the famous basketball team in this hometown:\n",
      "   The famous basketball team in Boston is the Boston Celtics.\n",
      "\n",
      "5. Determine the second name of this basketball team:\n",
      "   The second name of the team is \"Celtics\".\n",
      "\n",
      "Therefore, the answer to the original question \"What is the second name of this basketball team?\" is Celtics.\n"
     ]
    }
   ],
   "source": [
    "problem = \"\"\"\n",
    "Alex, a 35-year-old film enthusiast from Brazil, is watching a documentary about iconic movie soundtracks.\n",
    "He becomes captivated by a song from a film where a bride seeks revenge.\n",
    "Intrigued by the lead actress, he investigates her background and travels to her hometown.\n",
    "Once there, Alex’s attention shifts to the local sports culture.\n",
    "He learns about a famous basketball team known for its rich history and legacy.\n",
    "What is the second name of this basketball team?\n",
    "\"\"\"\n",
    "\n",
    "cot_query = f\"\"\"Problem statement:\n",
    "{problem}\n",
    "First, list systematically and in detail all the problems in this problem\n",
    "that need to be solved before we can arrive at the correct answer.\n",
    "Then, solve each sub problem using the answers of previous problems\n",
    "and reach a final solution.\n",
    "\"\"\"\n",
    "\n",
    "print(llm.invoke(cot_query).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5680541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the problem and identify the sub-problems that need to be solved:\n",
      "\n",
      "1. Identifying the words in \"New Hampshire\"\n",
      "2. Determining the last letter of each word\n",
      "3. Concatenating the last letters\n",
      "\n",
      "Now, let's solve each sub-problem:\n",
      "\n",
      "1. Identifying the words in \"New Hampshire\":\n",
      "   - The phrase \"New Hampshire\" consists of two words: \"New\" and \"Hampshire\"\n",
      "\n",
      "2. Determining the last letter of each word:\n",
      "   - For \"New\": The last letter is \"w\"\n",
      "   - For \"Hampshire\": The last letter is \"e\"\n",
      "\n",
      "3. Concatenating the last letters:\n",
      "   - We need to join the last letters we identified in step 2 in the order they appear in the original phrase\n",
      "\n",
      "Final solution:\n",
      "Now that we have solved each sub-problem, we can combine the results to get our final answer.\n",
      "\n",
      "The last letter of \"New\" is \"w\", and the last letter of \"Hampshire\" is \"e\".\n",
      "Concatenating these letters in order gives us: \"we\"\n",
      "\n",
      "Therefore, the final answer is \"we\".\n"
     ]
    }
   ],
   "source": [
    "problem = \"\"\"\n",
    "take the last letters of the words in \"New Hampshire\" and concatenate them.\n",
    "\"\"\"\n",
    "cot_query = f\"\"\"Problem statement:\n",
    "{problem}\n",
    "First, list systematically and in detail all the problems in this problem that need to be solved before we can arrive at the correct answer. Then, solve each sub problem using the answers of previous problems and reach a final solution.\n",
    "\"\"\"\n",
    "\n",
    "print(llm.invoke(cot_query).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2784e1",
   "metadata": {},
   "source": [
    "### Forcing models to be consistent by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c024a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's approach this step-by-step:\n",
      "\n",
      "1. When you were 6, your sister was half your age.\n",
      "   Half of 6 is 3, so your sister was 3 years old when you were 6.\n",
      "\n",
      "2. This means your sister is 3 years younger than you.\n",
      "\n",
      "3. Now you're 70 years old.\n",
      "\n",
      "4. To find your sister's current age, we subtract the age difference:\n",
      "   70 - 3 = 67\n",
      "\n",
      "Therefore, your sister is 67 years old now.\n"
     ]
    }
   ],
   "source": [
    "template = f\"\"\"\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A: \n",
    "\"\"\"\n",
    "print(llm.invoke(template).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07c172",
   "metadata": {},
   "source": [
    "### Tree of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3248d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how three different experts might approach this problem step-by-step:\n",
      "\n",
      "Expert 1 (Mathematician):\n",
      "Step 1: Let's define variables. Let x be your current age and y be your sister's current age.\n",
      "\n",
      "Expert 2 (Logician):\n",
      "Step 1: We need to establish the relationship between your age and your sister's age when you were 6.\n",
      "\n",
      "Expert 3 (Problem Solver):\n",
      "Step 1: First, let's focus on the key information: when you were 6, your sister was half your age.\n",
      "\n",
      "Expert 1:\n",
      "Step 2: When you were 6, your sister was half your age. So we can write: y - 64 = (6) / 2 = 3\n",
      "\n",
      "Expert 2:\n",
      "Step 2: If you were 6 and she was half your age, then she was 3 years old at that time.\n",
      "\n",
      "Expert 3:\n",
      "Step 2: This means when you were 6, your sister was 3 years old.\n",
      "\n",
      "Expert 1:\n",
      "Step 3: Now we know that x = 70 (your current age) and y - 64 = 3. Solving for y: y = 67\n",
      "\n",
      "Expert 2:\n",
      "Step 3: The age difference between you and your sister is constant. It's 6 - 3 = 3 years.\n",
      "\n",
      "Expert 3:\n",
      "Step 3: The age gap between you and your sister is 3 years, and this gap remains constant throughout life.\n",
      "\n",
      "Expert 1:\n",
      "Step 4: Therefore, your sister is currently 67 years old.\n",
      "\n",
      "Expert 2:\n",
      "Step 4: If you're now 70, and you're always 3 years older, your sister must be 70 - 3 = 67 years old.\n",
      "\n",
      "Expert 3:\n",
      "Step 4: Since you're now 70, your sister must be 70 - 3 = 67 years old.\n",
      "\n",
      "All experts have reached the same conclusion: Your sister is 67 years old.\n"
     ]
    }
   ],
   "source": [
    "template = f\"\"\"\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is {\"when I was 6 my sister was half my age. Now I’m 70 how old is my sister?\"}\n",
    " \n",
    "\"\"\"\n",
    "print(llm.invoke(template).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a9b1c",
   "metadata": {},
   "source": [
    "### Using external functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "621fecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbore/coding/learning/langchain-workshops/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "/Users/sbore/coding/learning/langchain-workshops/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer this question, I'll need to search for information about Albert Einstein and his death. Let's start with a general search query.\n",
      "\n",
      "Action: duckduckgo_search\n",
      "Action Input: Albert Einstein biography and death\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAlbert Einstein (born March 14, 1879, Ulm, Württemberg, Germany—died April 18, 1955, Princeton, New Jersey, U.S.) was a German-born physicist who developed the special and general theories of relativity and won the Nobel Prize for Physics in 1921 for his explanation of the photoelectric effect.Einstein is generally considered the most influential physicist of the 20th century. A brief biography of Albert Einstein (March 14, 1879 - April 18, 1955), the scientist whose theories changed the way we think about the universe. ... Einstein's later years and death. Albert Einstein Death. Albert Einstein passed away on April 18, 1955, in Princeton, New Jersey, USA. He died at the age of 76 from an abdominal aortic aneurysm, a condition where the large blood vessel that supplies blood to the abdomen, pelvis, and legs becomes unusually large or balloons outward. Despite experiencing severe pain associated ... But shortly before to his death in 1955, Einstein reflected on his life and identified his single greatest regret: his role in the development of the atomic bomb. Albert Einstein - Physics, Relativity, Nobel Prize: In some sense, Einstein, instead of being a relic, may have been too far ahead of his time. The strong force, a major piece of any unified field theory, was still a total mystery in Einstein's lifetime. Only in the 1970s and '80s did physicists begin to unravel the secret of the strong force with the quark model.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Based on the search results, I now have the information to answer the question about Albert Einstein and his age at the time of his death.\n",
      "\n",
      "Final Answer: Albert Einstein was a German-born physicist who is widely considered the most influential physicist of the 20th century. He developed the special and general theories of relativity and won the Nobel Prize for Physics in 1921 for his explanation of the photoelectric effect. Einstein was born on March 14, 1879, in Ulm, Württemberg, Germany, and died on April 18, 1955, in Princeton, New Jersey, USA. At the time of his death, Albert Einstein was 76 years old. He died from an abdominal aortic aneurysm.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Albert Einstein was a German-born physicist who is widely considered the most influential physicist of the 20th century. He developed the special and general theories of relativity and won the Nobel Prize for Physics in 1921 for his explanation of the photoelectric effect. Einstein was born on March 14, 1879, in Ulm, Württemberg, Germany, and died on April 18, 1955, in Princeton, New Jersey, USA. At the time of his death, Albert Einstein was 76 years old. He died from an abdominal aortic aneurysm.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "llm = BedrockChat(model_id=model_id, client=bedrock_runtime, model_kwargs={\"temperature\": 0.9})\n",
    "\n",
    "tools = load_tools([\"ddg-search\"], llm=llm)\n",
    "# Initializing an agent\n",
    "agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# Testing\n",
    "agent.run(\"Who is Albert Einstein and what was his age at the time of his death?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9a0a6",
   "metadata": {},
   "source": [
    "### Anonimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b64faec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version as a short, official email:\n",
      "\n",
      "Subject: Lost Wallet - Slim Shady\n",
      "\n",
      "Dear Sir/Madam,\n",
      "\n",
      "We are writing to inform you that Slim Shady has recently lost his wallet. If found, please contact us using the information below:\n",
      "\n",
      "Phone: 313-666-7440\n",
      "Email: real.slim.shady@gmail.com\n",
      "\n",
      "Thank you for your assistance in this matter.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "[Your Position/Organization]\n",
      "\n",
      "Note: I've intentionally omitted specific details about the wallet's contents, including the credit card number, for security reasons. It's generally not advisable to share such sensitive information in emails or public notices.\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer\n",
    "\n",
    "\n",
    "text = \"\"\"Slim Shady recently lost his wallet.\n",
    "Inside is some cash and his credit card with the number 4916 0387 9536 0861.\n",
    "If you would find it, please call at 313-666-7440 or write an email here: real.slim.shady@gmail.com.\"\"\"\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer()\n",
    "\n",
    "template = \"\"\"Rewrite this text into an official, short email:\n",
    "\n",
    "{anonymized_text}\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\"anonymized_text\": anonymizer.anonymize} | prompt | llm | (lambda ai_message: anonymizer.deanonymize(ai_message.content))\n",
    "response = chain.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2e544",
   "metadata": {},
   "source": [
    "### How to rag - less and more advanced approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1156dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Easy_recipes.pdf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "pdf_url =  \"https://www.bu.edu/geneva/files/2010/08/Easy_recipes.pdf\"\n",
    "wget.download(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e3d14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "persist_directory = \"./storage25\"\n",
    "pdf_path = \"Easy_recipes.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=10)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11b93420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(str_):\n",
    "  return str_.replace(\"\\xa0\", \"\", -1).replace(\"\\t\", \"\", -1).replace(\"\\r\", \"\", -1)\n",
    "for text in texts:\n",
    "  text.page_content = normalize_text(text.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072db314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'model=\"curie\"'\\\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=texts,\n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd2146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Chilli\"\n",
    "docs = vectordb.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d29cd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "#  Discuss predictable scaling\n",
    "while True:\n",
    "        user_input = input(\"Enter a query: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        query = f\"###Prompt {user_input}\"\n",
    "        try:\n",
    "            llm_response = qa(query)\n",
    "            print(llm_response[\"result\"])\n",
    "        except Exception as err:\n",
    "            print('Exception occurred. Please try again', str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4560fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f3ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "system_prompt = system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d9ab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but the given context does not provide information about common ways of preparing chilli con carne. The text only mentions that it's an easy recipe to learn and cook, but doesn't give details about specific preparation methods. To answer this question accurately, I would need more information about cooking techniques for chilli con carne.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Chilli con carne?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173776f",
   "metadata": {},
   "source": [
    "### Graph Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3db1421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI, PASSWORD, USERNAME = ('neo4j+s://0ba11fa2.databases.neo4j.io',\n",
    " 't2ciXjyUuNXCK3KR3sxTQ2Rq8LJSyTKfHx6sn0ZOzmo',\n",
    " 'neo4j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "136194b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b5ca03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "with GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD)) as driver: \n",
    "    driver.verify_connectivity() \n",
    "    print(\"Connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca9d6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(driver, query, parameters = None):\n",
    "    \"\"\"\n",
    "    Executes a Cypher query on Neo4j and returns the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open a session with Neo4j\n",
    "        with driver.session() as session:\n",
    "            # Run the query within a transaction\n",
    "            result = session.run(query, parameters)\n",
    "            # Extract and return results as a list of dictionaries\n",
    "            return [record.data() for record in result]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0376760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/5tdxg0g979ld_8q10pg9t_hx40q9qn/T/ipykernel_93639/1537147728.py:7: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  with driver.session() as session:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'count(m)': 9}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_query(driver, \n",
    "    \"\"\" LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/Foutse/Neo4j_experiences/main/dataset_movie/small_grouped_data.csv' AS l FIELDTERMINATOR ',' \n",
    "CREATE(m:MovieLens{M_movieId:toInteger(l.movieId), M_rating:apoc.convert.fromJsonList(l.rating),M_userId:apoc.convert.fromJsonList(l.userId), M_timestamp:l.timestamp,M_title:l.title,M_summary:l.summary,M_year:toInteger(l.movie_year),M_genres:apoc.convert.fromJsonList(l.genres), M_Avg_rating:toFloat(l.Avg_rating),M_unique_tags:apoc.convert.fromJsonList(l.unique_tags), M_unique_Uid_tags:apoc.convert.fromJsonList(l.unique_Uid_tags),M_round_ratings:apoc.convert.fromJsonList(l.round_ratings), M_rating_vector:apoc.convert.fromJsonList(l.rating_vector),M_round_ratingsup:apoc.convert.fromJsonList(l.round_ratingsup), M_round_ratingsdown:apoc.convert.fromJsonList(l.round_ratingsdown),M_rating_vectorup:apoc.convert.fromJsonList(l.rating_vectorup), M_rating_vectordown:apoc.convert.fromJsonList(l.rating_vectordown)}) \n",
    "RETURN count(m)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba45dfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>userId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>Avg_rating</th>\n",
       "      <th>unique_tags</th>\n",
       "      <th>unique_Uid_tags</th>\n",
       "      <th>round_ratings</th>\n",
       "      <th>rating_vector</th>\n",
       "      <th>round_ratingsup</th>\n",
       "      <th>round_ratingsdown</th>\n",
       "      <th>rating_vectorup</th>\n",
       "      <th>rating_vectordown</th>\n",
       "      <th>summary</th>\n",
       "      <th>movie_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125974</td>\n",
       "      <td>[3.5, 4.0]</td>\n",
       "      <td>[89, 596]</td>\n",
       "      <td>[datetime.date(2018, 3, 7), datetime.date(2018...</td>\n",
       "      <td>X-Men: Apocalypse (2016)</td>\n",
       "      <td>['Action', 'Adventure', 'Fantasy', 'Sci-Fi']</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>[4.0, 4.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>Summary not found</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138610</td>\n",
       "      <td>[1.5]</td>\n",
       "      <td>[610]</td>\n",
       "      <td>[datetime.date(2017, 5, 3)]</td>\n",
       "      <td>Batman v Superman: Dawn of Justice (2016)</td>\n",
       "      <td>['Action', 'Adventure', 'Fantasy', 'Sci-Fi']</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>20 years after a horrific accident during a sm...</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153070</td>\n",
       "      <td>[3.5]</td>\n",
       "      <td>[567]</td>\n",
       "      <td>[datetime.date(2018, 5, 2)]</td>\n",
       "      <td>Gods of Egypt (2016)</td>\n",
       "      <td>['Adventure', 'Fantasy']</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>['nightmare']</td>\n",
       "      <td>[567]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>Summary not found</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167746</td>\n",
       "      <td>[3.5, 4.0, 4.0, 5.0, 1.0, 3.5, 5.0]</td>\n",
       "      <td>[111, 125, 249, 380, 517, 567, 596]</td>\n",
       "      <td>[datetime.date(2018, 1, 20), datetime.date(201...</td>\n",
       "      <td>Split (2017)</td>\n",
       "      <td>['Drama', 'Horror', 'Thriller']</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>['funny', 'heartwarming']</td>\n",
       "      <td>[567]</td>\n",
       "      <td>[4, 4, 4, 5, 1, 4, 5]</td>\n",
       "      <td>[0.14285714285714285, 0.0, 0.0, 0.571428571428...</td>\n",
       "      <td>[4, 4, 4, 5, 1, 4, 5]</td>\n",
       "      <td>[4.0, 4.0, 4.0, 5.0, 1.0, 4.0, 5.0]</td>\n",
       "      <td>[0.14285714285714285, 0.0, 0.0, 0.571428571428...</td>\n",
       "      <td>[0.14285714285714285, 0.0, 0.0, 0.571428571428...</td>\n",
       "      <td>A cooler-than-ever Bruce Wayne must deal with ...</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170551</td>\n",
       "      <td>[2.5]</td>\n",
       "      <td>[599]</td>\n",
       "      <td>[datetime.date(2017, 6, 26)]</td>\n",
       "      <td>Fifty Shades Darker (2017)</td>\n",
       "      <td>['Drama', 'Romance']</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>Summary not found</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>170945</td>\n",
       "      <td>[3.5]</td>\n",
       "      <td>[567]</td>\n",
       "      <td>[datetime.date(2018, 5, 2)]</td>\n",
       "      <td>Dave Chappelle: The Age of Spin (2017)</td>\n",
       "      <td>['Comedy']</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>['Suspenseful', 'paranoia']</td>\n",
       "      <td>[567]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>Secure within a desolate home as an unnatural ...</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>184471</td>\n",
       "      <td>[3.5, 0.5, 3.0, 3.0]</td>\n",
       "      <td>[62, 153, 380, 514]</td>\n",
       "      <td>[datetime.date(2018, 6, 3), datetime.date(2018...</td>\n",
       "      <td>The Commuter (2018)</td>\n",
       "      <td>['Crime', 'Drama', 'Mystery', 'Thriller']</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>['Alicia Vikander', 'adventure', 'video game a...</td>\n",
       "      <td>[62]</td>\n",
       "      <td>[4, 1, 3, 3]</td>\n",
       "      <td>[0.25, 0.0, 0.5, 0.25, 0.0]</td>\n",
       "      <td>[4, 1, 3, 3]</td>\n",
       "      <td>[4.0, 0.0, 3.0, 3.0]</td>\n",
       "      <td>[0.25, 0.0, 0.5, 0.25, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.5, 0.25, 0.0]</td>\n",
       "      <td>Lara Croft, the fiercely independent daughter ...</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>187595</td>\n",
       "      <td>[4.0, 4.0, 3.5, 3.0, 5.0]</td>\n",
       "      <td>[62, 380, 414, 514, 586]</td>\n",
       "      <td>[datetime.date(2018, 6, 14), datetime.date(201...</td>\n",
       "      <td>Tomb Raider (2018)</td>\n",
       "      <td>['Action', 'Adventure', 'Fantasy']</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>['Emilia Clarke', 'star wars']</td>\n",
       "      <td>[62]</td>\n",
       "      <td>[4, 4, 4, 3, 5]</td>\n",
       "      <td>[0.0, 0.0, 0.2, 0.6, 0.2]</td>\n",
       "      <td>[4, 4, 4, 3, 5]</td>\n",
       "      <td>[4.0, 4.0, 4.0, 3.0, 5.0]</td>\n",
       "      <td>[0.0, 0.0, 0.2, 0.6, 0.2]</td>\n",
       "      <td>[0.0, 0.0, 0.2, 0.6, 0.2]</td>\n",
       "      <td>Board the Millennium Falcon and journey to a g...</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>193565</td>\n",
       "      <td>[3.5]</td>\n",
       "      <td>[184]</td>\n",
       "      <td>[datetime.date(2018, 9, 16)]</td>\n",
       "      <td>Dogman (2018)</td>\n",
       "      <td>['Crime', 'Drama']</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>['anime', 'comedy', 'gintama', 'remaster']</td>\n",
       "      <td>[184]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>The silver-haired samurai Sakata Gintoki inves...</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               rating  \\\n",
       "0   125974                           [3.5, 4.0]   \n",
       "1   138610                                [1.5]   \n",
       "2   153070                                [3.5]   \n",
       "3   167746  [3.5, 4.0, 4.0, 5.0, 1.0, 3.5, 5.0]   \n",
       "4   170551                                [2.5]   \n",
       "5   170945                                [3.5]   \n",
       "6   184471                 [3.5, 0.5, 3.0, 3.0]   \n",
       "7   187595            [4.0, 4.0, 3.5, 3.0, 5.0]   \n",
       "8   193565                                [3.5]   \n",
       "\n",
       "                                userId  \\\n",
       "0                            [89, 596]   \n",
       "1                                [610]   \n",
       "2                                [567]   \n",
       "3  [111, 125, 249, 380, 517, 567, 596]   \n",
       "4                                [599]   \n",
       "5                                [567]   \n",
       "6                  [62, 153, 380, 514]   \n",
       "7             [62, 380, 414, 514, 586]   \n",
       "8                                [184]   \n",
       "\n",
       "                                           timestamp  \\\n",
       "0  [datetime.date(2018, 3, 7), datetime.date(2018...   \n",
       "1                        [datetime.date(2017, 5, 3)]   \n",
       "2                        [datetime.date(2018, 5, 2)]   \n",
       "3  [datetime.date(2018, 1, 20), datetime.date(201...   \n",
       "4                       [datetime.date(2017, 6, 26)]   \n",
       "5                        [datetime.date(2018, 5, 2)]   \n",
       "6  [datetime.date(2018, 6, 3), datetime.date(2018...   \n",
       "7  [datetime.date(2018, 6, 14), datetime.date(201...   \n",
       "8                       [datetime.date(2018, 9, 16)]   \n",
       "\n",
       "                                       title  \\\n",
       "0                   X-Men: Apocalypse (2016)   \n",
       "1  Batman v Superman: Dawn of Justice (2016)   \n",
       "2                       Gods of Egypt (2016)   \n",
       "3                               Split (2017)   \n",
       "4                 Fifty Shades Darker (2017)   \n",
       "5     Dave Chappelle: The Age of Spin (2017)   \n",
       "6                        The Commuter (2018)   \n",
       "7                         Tomb Raider (2018)   \n",
       "8                              Dogman (2018)   \n",
       "\n",
       "                                         genres  Avg_rating  \\\n",
       "0  ['Action', 'Adventure', 'Fantasy', 'Sci-Fi']    3.750000   \n",
       "1  ['Action', 'Adventure', 'Fantasy', 'Sci-Fi']    1.500000   \n",
       "2                      ['Adventure', 'Fantasy']    3.500000   \n",
       "3               ['Drama', 'Horror', 'Thriller']    3.714286   \n",
       "4                          ['Drama', 'Romance']    2.500000   \n",
       "5                                    ['Comedy']    3.500000   \n",
       "6     ['Crime', 'Drama', 'Mystery', 'Thriller']    2.500000   \n",
       "7            ['Action', 'Adventure', 'Fantasy']    3.900000   \n",
       "8                            ['Crime', 'Drama']    3.500000   \n",
       "\n",
       "                                         unique_tags unique_Uid_tags  \\\n",
       "0                                                 []              []   \n",
       "1                                                 []              []   \n",
       "2                                      ['nightmare']           [567]   \n",
       "3                          ['funny', 'heartwarming']           [567]   \n",
       "4                                                 []              []   \n",
       "5                        ['Suspenseful', 'paranoia']           [567]   \n",
       "6  ['Alicia Vikander', 'adventure', 'video game a...            [62]   \n",
       "7                     ['Emilia Clarke', 'star wars']            [62]   \n",
       "8         ['anime', 'comedy', 'gintama', 'remaster']           [184]   \n",
       "\n",
       "           round_ratings                                      rating_vector  \\\n",
       "0                 [4, 4]                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "1                    [2]                          [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "2                    [4]                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "3  [4, 4, 4, 5, 1, 4, 5]  [0.14285714285714285, 0.0, 0.0, 0.571428571428...   \n",
       "4                    [3]                          [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "5                    [4]                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "6           [4, 1, 3, 3]                        [0.25, 0.0, 0.5, 0.25, 0.0]   \n",
       "7        [4, 4, 4, 3, 5]                          [0.0, 0.0, 0.2, 0.6, 0.2]   \n",
       "8                    [4]                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "\n",
       "         round_ratingsup                    round_ratingsdown  \\\n",
       "0                 [4, 4]                           [4.0, 4.0]   \n",
       "1                    [2]                                [2.0]   \n",
       "2                    [4]                                [4.0]   \n",
       "3  [4, 4, 4, 5, 1, 4, 5]  [4.0, 4.0, 4.0, 5.0, 1.0, 4.0, 5.0]   \n",
       "4                    [3]                                [2.0]   \n",
       "5                    [4]                                [4.0]   \n",
       "6           [4, 1, 3, 3]                 [4.0, 0.0, 3.0, 3.0]   \n",
       "7        [4, 4, 4, 3, 5]            [4.0, 4.0, 4.0, 3.0, 5.0]   \n",
       "8                    [4]                                [4.0]   \n",
       "\n",
       "                                     rating_vectorup  \\\n",
       "0                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "1                          [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "2                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "3  [0.14285714285714285, 0.0, 0.0, 0.571428571428...   \n",
       "4                          [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "5                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "6                        [0.25, 0.0, 0.5, 0.25, 0.0]   \n",
       "7                          [0.0, 0.0, 0.2, 0.6, 0.2]   \n",
       "8                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "\n",
       "                                   rating_vectordown  \\\n",
       "0                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "1                          [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "2                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "3  [0.14285714285714285, 0.0, 0.0, 0.571428571428...   \n",
       "4                          [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "5                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "6                         [0.0, 0.0, 0.5, 0.25, 0.0]   \n",
       "7                          [0.0, 0.0, 0.2, 0.6, 0.2]   \n",
       "8                          [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "\n",
       "                                             summary  movie_year  \n",
       "0                                  Summary not found      2016.0  \n",
       "1  20 years after a horrific accident during a sm...      2016.0  \n",
       "2                                  Summary not found      2016.0  \n",
       "3  A cooler-than-ever Bruce Wayne must deal with ...      2017.0  \n",
       "4                                  Summary not found      2017.0  \n",
       "5  Secure within a desolate home as an unnatural ...      2017.0  \n",
       "6  Lara Croft, the fiercely independent daughter ...      2018.0  \n",
       "7  Board the Millennium Falcon and journey to a g...      2018.0  \n",
       "8  The silver-haired samurai Sakata Gintoki inves...      2018.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"https://raw.githubusercontent.com/Foutse/Neo4j_experiences/main/dataset_movie/small_grouped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "484d2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "753e6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=URI, username=USERNAME, password=PASSWORD\n",
    ")\n",
    "chain = GraphCypherQAChain.from_llm(llm, graph=graph, verbose=False, return_intermediate_steps=True, allow_dangerous_requests=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "631f1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke(\"what is the highest rated movie of all time? Provide me with the description of the movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6761c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (m:MovieLens)\n",
      "WHERE m.M_Avg_rating IS NOT NULL\n",
      "WITH m ORDER BY m.M_Avg_rating DESC LIMIT 1\n",
      "RETURN m.M_title AS Title, m.M_Avg_rating AS Rating, m.M_summary AS Description\n"
     ]
    }
   ],
   "source": [
    "print(res['intermediate_steps'][0]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d57d9a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [{'Title': 'Tomb Raider (2018)', 'Rating': 3.9, 'Description': \"Board the Millennium Falcon and journey to a galaxy far, far away in an epic action-adventure that will set the course of one of the Star Wars saga's most unlikely heroes.\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(res['intermediate_steps'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71509398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, I can't determine the highest rated movie of all time. However, I can tell you about one movie that has been rated. \"Tomb Raider\" from 2018 has a rating of 3.9 out of 5. Interestingly, the description provided seems to be for a different movie, possibly a Star Wars film. It reads: \"Board the Millennium Falcon and journey to a galaxy far, far away in an epic action-adventure that will set the course of one of the Star Wars saga's most unlikely heroes.\"\n"
     ]
    }
   ],
   "source": [
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab25c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddab4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_dolby_wiki(url = \"https://en.wikipedia.org/wiki/Dolby\"):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page with status code {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the title of the page\n",
    "    text = soup.text.strip().replace(\"\\n\\n\", \"\", -1)\n",
    "    return text\n",
    "\n",
    "text = scrape_dolby_wiki()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cee7732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Dolby Vision', type='Technology', properties={}), Node(id='Dolby Theatre', type='Location', properties={}), Node(id='Ray Dolby', type='Person', properties={}), Node(id='Dolby Surround', type='Technology', properties={}), Node(id='Dolby Noise Reduction system', type='Technology', properties={}), Node(id='ATSC standards', type='Standard', properties={}), Node(id='Dolby TrueHD', type='Technology', properties={}), Node(id='Dolby Atmos', type='Technology', properties={}), Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), Node(id='Dolby Digital', type='Technology', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Ray Dolby', type='Person', properties={}), target=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), type='FOUNDED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby Noise Reduction system', type='Technology', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby Digital', type='Technology', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby Atmos', type='Technology', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby Vision', type='Technology', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby Surround', type='Technology', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby TrueHD', type='Technology', properties={}), type='CREATED', properties={}), Relationship(source=Node(id='Dolby Laboratories, Inc.', type='Company', properties={}), target=Node(id='Dolby Theatre', type='Location', properties={}), type='OWNS', properties={}), Relationship(source=Node(id='Dolby Digital', type='Technology', properties={}), target=Node(id='ATSC standards', type='Standard', properties={}), type='USED_IN', properties={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35bb40e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='Dolby Vision' type='Technology' properties={}\n",
      "id='Dolby Theatre' type='Location' properties={}\n",
      "id='Ray Dolby' type='Person' properties={}\n",
      "id='Dolby Surround' type='Technology' properties={}\n",
      "id='Dolby Noise Reduction system' type='Technology' properties={}\n",
      "id='ATSC standards' type='Standard' properties={}\n",
      "id='Dolby TrueHD' type='Technology' properties={}\n",
      "id='Dolby Atmos' type='Technology' properties={}\n",
      "id='Dolby Laboratories, Inc.' type='Company' properties={}\n",
      "id='Dolby Digital' type='Technology' properties={}\n"
     ]
    }
   ],
   "source": [
    "for node in graph_documents[0].nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e49fb5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7b65c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = GraphCypherQAChain.from_llm(llm, graph=graph, verbose=False, return_intermediate_steps=True, allow_dangerous_requests=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41e0620f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is Ray Dolby?',\n",
       " 'result': 'Ray Dolby is the founder of Dolby Laboratories, Inc. He established this renowned company, which has become a significant player in audio technology and innovation.',\n",
       " 'intermediate_steps': [{'query': 'MATCH (p:Person {id: \"Ray Dolby\"})-[:FOUNDED]->(o:Organization)\\nRETURN p.id AS Person, COLLECT(o.id) AS FoundedOrganizations'},\n",
       "  {'context': [{'Person': 'Ray Dolby',\n",
       "     'FoundedOrganizations': ['Dolby Laboratories, Inc.']}]}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is Ray Dolby?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7cf0a3",
   "metadata": {},
   "source": [
    "### Aditional resources\n",
    "* [Multimodal rag example](https://colab.research.google.com/drive/15_IkDc_zV8VFqe6LXwNHkkflgc9RKb5k?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4f267",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
